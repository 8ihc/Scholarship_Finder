"""
Cloud OCR è™•ç†å™¨
æ­¤è…³æœ¬è™•ç†ç¶“ document_parsing and OCR staging.py æ¨™è¨˜ç‚º 'OCR_REQUIRED' çš„æª”æ¡ˆã€‚
å®ƒåˆ©ç”¨ Google Cloud Vision API çš„éåŒæ­¥æ‰¹æ¬¡è™•ç†åŠŸèƒ½ä¾†è™•ç† PDF å’Œåœ–åƒï¼Œ
ä¸¦å°‡ OCR çµæœå›å¡«åˆ° scholarships_parsed_texts.json ä¸­ã€‚
"""

import os
import re
import json
import time
import argparse
import logging
import unicodedata
from pathlib import Path
from google.cloud import storage, vision
from google.longrunning import operations_pb2
from google.api_core import operation as longrunning
from typing import List, Dict, Any

# --- Configuration (è«‹æ ¹æ“šæ‚¨çš„ç’°å¢ƒè¨­å®š) ---
# æœå‹™å¸³æˆ¶é‡‘é‘°æª”æ¡ˆè·¯å¾‘
SERVICE_ACCOUNT_FILE = "C:\\Users\\8ihc8\\Desktop\\new_scholarship\\service-account-key.json.json"

# Google Cloud Storage Bucket åç¨± (å¿…é ˆé å…ˆå‰µå»º)
GCS_BUCKET_NAME = "ntu-scholarship-ocr-taipei-2025" 
GCS_INPUT_FOLDER = "ocr_input/"
GCS_OUTPUT_FOLDER = "ocr_output/"

# è«‹å°‡æ‚¨çš„å°ˆæ¡ˆ ID å¡«å…¥æ­¤è™•
PROJECT_ID = "steady-dryad-478107" 

# æª”æ¡ˆè·¯å¾‘è¨­å®š
PARSED_INPUT_FILE = Path("data/processed/scholarships_parsed_texts.json")
# ---------------------------------------------

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def upload_to_gcs(storage_client: storage.Client, local_file_path: Path, gcs_destination: str) -> str:
    """å°‡æœ¬åœ°æª”æ¡ˆä¸Šå‚³åˆ° GCS"""
    bucket = storage_client.bucket(GCS_BUCKET_NAME)
    blob = bucket.blob(gcs_destination)
    
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå·²å­˜åœ¨å‰‡è·³éä¸Šå‚³ (é¿å…é‡è¤‡æ”¶è²»)
    if blob.exists():
        logging.info(f"æª”æ¡ˆå·²å­˜åœ¨ GCS: {gcs_destination}. è·³éä¸Šå‚³.")
        return f"gs://{GCS_BUCKET_NAME}/{gcs_destination}"

    blob.upload_from_filename(local_file_path)
    logging.info(f"æª”æ¡ˆä¸Šå‚³æˆåŠŸ: {local_file_path} -> {gcs_destination}")
    return f"gs://{GCS_BUCKET_NAME}/{gcs_destination}"

def async_batch_annotate_file(
    vision_client: vision.ImageAnnotatorClient, gcs_source_uri: str, gcs_destination_uri: str
) -> longrunning.Operation:
    """å° GCS ä¸Šçš„ PDF/TIFF/åœ–ç‰‡åŸ·è¡ŒéåŒæ­¥æ‰¹æ¬¡ OCR"""
    
    # æ”¯æ´çš„åœ–ç‰‡å’Œæª”æ¡ˆé¡å‹
    lower_uri = gcs_source_uri.lower()
    if lower_uri.endswith(('.pdf', '.tif', '.tiff')):
        mime_type = "application/pdf"
    elif lower_uri.endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):
        # å°å–®å¼µå½±åƒæª”ï¼Œè«‹è¨­å®šæ­£ç¢ºçš„ image/* MIME é¡å‹ï¼Œ
        # è€Œä¸æ˜¯ä¸€å¾‹ä½¿ç”¨ image/gifï¼ˆé‚£æœƒå°è‡´ jpeg/png ç„¡æ³•è¢«æ­£ç¢ºè™•ç†ï¼‰ã€‚
        filename = lower_uri.split('/')[-1]
        _, ext = os.path.splitext(filename)
        if ext == '.png':
            mime_type = 'image/png'
        elif ext in ('.jpg', '.jpeg'):
            mime_type = 'image/jpeg'
        elif ext == '.gif':
            mime_type = 'image/gif'
        elif ext == '.bmp':
            mime_type = 'image/bmp'
        else:
            mime_type = 'image/*'
        logging.info(f"å½±åƒæª”åµæ¸¬: è¨­å®š MIME é¡å‹ {mime_type}ï¼Œä¾†æº: {gcs_source_uri}")
    else:
        # ä¸æ‡‰ç™¼ç”Ÿï¼Œå› ç‚º document_parser å·²ç¶“ç¯©é¸é
        logging.warning(f"æœªçŸ¥ MIME é¡å‹ï¼Œè·³é OCR: {gcs_source_uri}")
        return None

    input_config = vision.InputConfig(
        gcs_source=vision.GcsSource(uri=gcs_source_uri),
        mime_type=mime_type,
    )
    
    output_config = vision.OutputConfig(
        gcs_destination=vision.GcsDestination(uri=gcs_destination_uri),
        batch_size=20, # æ¯æ¬¡æ‰¹æ¬¡è™•ç†çš„é é¢æ•¸ (åƒ…é™ PDF/TIFF)
    )

    feature = vision.Feature(type_=vision.Feature.Type.DOCUMENT_TEXT_DETECTION)

    # å‰µå»ºå–®ä¸€æª”æ¡ˆçš„ Annotate Request
    annotate_request = vision.AsyncAnnotateFileRequest(
        input_config=input_config,
        features=[feature],
        output_config=output_config,
    )
    
    # å°‡ Annotate Request æ”¾å…¥åˆ—è¡¨ä¸­ï¼Œä½œç‚º requests åƒæ•¸çš„å€¼
    # é€™æ˜¯ vision_client.async_batch_annotate_files å‡½æ•¸çš„æ­£ç¢ºè¼¸å…¥æ ¼å¼
    
    logging.info(f"ç™¼é€éåŒæ­¥ OCR è«‹æ±‚: {gcs_source_uri}")
    return vision_client.async_batch_annotate_files(requests=[annotate_request]) # <--- å°‡ annotate_request æ”¾å…¥åˆ—è¡¨ä¸­

def download_ocr_results(storage_client: storage.Client, gcs_output_uri: str) -> str:
    """å¾ GCS ä¸‹è¼‰ OCR è¼¸å‡ºçš„ JSON æ–‡ä»¶ä¸¦åˆä½µæ–‡æœ¬"""
    
    bucket_name = gcs_output_uri.split('/')[2]
    prefix = '/'.join(gcs_output_uri.split('/')[3:])
    
    bucket = storage_client.bucket(bucket_name)
    blobs = list(bucket.list_blobs(prefix=prefix))
    
    full_text = []
    
    # OCR è¼¸å‡ºçµæœæœƒæœ‰å¤šå€‹ JSON æª”æ¡ˆ (ä¾‹å¦‚ output-1-to-20.json)
    for blob in blobs:
        if not blob.name.endswith(".json"):
            continue
            
        logging.info(f"ä¸‹è¼‰ OCR çµæœ JSON: {blob.name}")
        json_string = blob.download_as_text(encoding="utf-8")
        response = json.loads(json_string)
        
        # åˆä½µæ‰€æœ‰é é¢çš„æ–‡å­—
        for page_response in response['responses']:
            text = page_response['fullTextAnnotation']['text']
            full_text.append(text)
            
    # æ¸…ç† GCS ä¸Šçš„è¼¸å‡ºæª”æ¡ˆ (å¯é¸ï¼Œä½†å»ºè­°æ¸…ç†ä»¥æ§åˆ¶å„²å­˜æˆæœ¬)
    # for blob in blobs:
    #     blob.delete()
        
    return "\n".join(full_text)


def validate_existing_ocr(storage_client: storage.Client, gcs_output_uri: str) -> str:
    """æª¢æŸ¥ GCS ä¸Šç¾æœ‰çš„ OCR JSON æ˜¯å¦åŒ…å«éç©ºæ–‡å­—ï¼›è‹¥åŒ…å«å‰‡å›å‚³åˆä½µå¾Œçš„æ–‡å­—ï¼Œå¦å‰‡å›å‚³ Noneã€‚"""
    bucket_name = gcs_output_uri.split('/')[2]
    prefix = '/'.join(gcs_output_uri.split('/')[3:])

    bucket = storage_client.bucket(bucket_name)
    blobs = list(bucket.list_blobs(prefix=prefix))

    merged_text_parts = []

    for blob in blobs:
        if not blob.name.endswith('.json'):
            continue

        try:
            logging.info(f"é©—è­‰ç¾æœ‰ OCR çµæœ JSON: {blob.name}")
            json_string = blob.download_as_text(encoding="utf-8")
            response = json.loads(json_string)

            for page_response in response.get('responses', []):
                text = page_response.get('fullTextAnnotation', {}).get('text', '')
                if text and text.strip():
                    merged_text_parts.append(text)

        except Exception as e:
            logging.debug(f"ç„¡æ³•è§£ææˆ–è®€å– {blob.name}: {e}")

    if merged_text_parts:
        return "\n".join(merged_text_parts)
    return None

def main_ocr_processor(only_ids: list = None, only_names: list = None):
    """ä¸»å‡½æ•¸ï¼šè™•ç† OCR æµç¨‹"""
    if GCS_BUCKET_NAME == "YOUR_GCS_BUCKET_NAME":
        logging.error("è«‹åœ¨è…³æœ¬ä¸­è¨­å®šæ­£ç¢ºçš„ GCS_BUCKET_NAMEã€‚")
        return
        
    if not PARSED_INPUT_FILE.exists():
        logging.error(f"æ‰¾ä¸åˆ°å·²è§£æçš„æª”æ¡ˆ {PARSED_INPUT_FILE}. è«‹å…ˆé‹è¡Œ document_parser.py")
        return

    # åˆå§‹åŒ–å®¢æˆ¶ç«¯ (ä½¿ç”¨æœå‹™å¸³æˆ¶é‡‘é‘°)
    storage_client = storage.Client.from_service_account_json(SERVICE_ACCOUNT_FILE)
    vision_client = vision.ImageAnnotatorClient.from_service_account_json(SERVICE_ACCOUNT_FILE)
    
    # 1. è®€å–è§£æçµæœ
    with open(PARSED_INPUT_FILE, 'r', encoding='utf-8') as f:
        parsed_data = json.load(f)

    def _needs_ocr(item: Dict[str, Any]) -> bool:
        # å¦‚æœ parsed_text ç‚ºç©ºï¼Œç„¡è«– status ç‚ºä½•ï¼Œéƒ½è¦–ç‚ºéœ€è¦ OCRï¼ˆå„ªå…ˆè™•ç†ï¼‰
        parsed = (item.get('parsed_text') or "").strip()
        if not parsed:
            logging.info(f"parsed_text ç‚ºç©ºï¼Œå°‡æ’å…¥ OCR: id={item.get('id')} name={item.get('name')} file={item.get('file_path_local')}")
            return True

        # å¦å‰‡ä¿ç•™åŸå…ˆçš„æ¨™è¨˜é‚è¼¯ï¼ˆè‹¥è¢«æ˜ç¢ºæ¨™ç‚º OCR_REQUIREDï¼‰
        if item.get('status') == 'OCR_REQUIRED':
            return True

        return False

    # å¦‚æœ only_ids/only_names è¢«æŒ‡å®šï¼Œåƒ…é‡å°é‚£äº›é …ç›®è™•ç†ã€‚
    def _is_allowed(item: Dict[str, Any]) -> bool:
        if not only_ids and not only_names:
            return True
        if only_ids and str(item.get('id')) in [str(x) for x in only_ids]:
            return True
        # ä½¿ç”¨ Unicode æ­£è¦åŒ– + casefold æ¯”å°æª”åç‰‡æ®µæˆ–æª”å
        if only_names:
            def _norm(s: str) -> str:
                return unicodedata.normalize('NFC', (s or '')).strip().casefold()

            lp_raw = item.get('file_path_local', '')
            lp_norm = _norm(lp_raw)
            for nm in only_names:
                nm_norm = _norm(nm)
                if nm_norm in lp_norm:
                    return True
                # æ¯”å° basename
                if _norm(Path(nm).name) == _norm(Path(lp_raw).name):
                    return True
        return False

    ocr_pending_files = [item for item in parsed_data if _needs_ocr(item) and _is_allowed(item)]

    if not ocr_pending_files:
        logging.info("æ²’æœ‰æ‰¾åˆ°éœ€è¦ OCR çš„æª”æ¡ˆã€‚æµç¨‹çµæŸã€‚")
        return

    logging.info(f"æ‰¾åˆ° {len(ocr_pending_files)} å€‹æª”æ¡ˆéœ€è¦ OCR è™•ç†ã€‚")

    # 2. åŸ·è¡Œ OCR æµç¨‹
    operations = []
    
    # 2a. ä¸Šå‚³æª”æ¡ˆä¸¦ç™¼èµ· OCR è«‹æ±‚
    for item in ocr_pending_files:
        local_path = Path(item['file_path_local'])

        # å¦‚æœåœ¨åŸå§‹ç›®éŒ„æ—æœ‰åŒå PDFï¼ˆä¾‹å¦‚ image.jpg -> image.pdfï¼‰ï¼Œå‰‡å„ªå…ˆä¸Šå‚³ PDF
        preferred_pdf = local_path.with_suffix('.pdf')
        if preferred_pdf.exists():
            upload_path = preferred_pdf
            logging.info(f"æ‰¾åˆ°åŒå PDFï¼Œå°‡ä¸Šå‚³ PDF è€ŒéåŸå§‹æª”: {preferred_pdf}")
        else:
            upload_path = local_path

        # GCS è¼¸å…¥è·¯å¾‘: ocr_input/[id]_[name].[ext]
        gcs_input_blob = f"{GCS_INPUT_FOLDER}{upload_path.name}"
        gcs_source_uri = upload_to_gcs(storage_client, upload_path, gcs_input_blob)
        
        # GCS è¼¸å‡ºè·¯å¾‘: ocr_output/[id]_[name]_output/
        gcs_output_uri_base = f"gs://{GCS_BUCKET_NAME}/{GCS_OUTPUT_FOLDER}{local_path.stem}_output/"
        
        # å¦‚æœ GCS ä¸Šå·²ç¶“æœ‰ OCR è¼¸å‡ºï¼Œå…ˆæª¢æŸ¥æ˜¯å¦åŒ…å«å¯¦éš›æ–‡å­—ï¼›è‹¥åŒ…å«å‰‡ç›´æ¥å›å¡«ï¼Œå¦å‰‡é‡æ–°ç™¼é€ OCR è«‹æ±‚
        bucket = storage_client.bucket(GCS_BUCKET_NAME)
        existing_blobs = list(bucket.list_blobs(prefix=f"{GCS_OUTPUT_FOLDER}{local_path.stem}_output/"))
        if any(b.name.endswith('.json') for b in existing_blobs):
            logging.info(f"æ‰¾åˆ°ç¾æœ‰ OCR è¼¸å‡ºï¼Œå˜—è©¦é©—è­‰å…§å®¹: {gcs_output_uri_base}")
            try:
                validated_text = validate_existing_ocr(storage_client, gcs_output_uri_base)
                if validated_text:
                    logging.info(f"ç¾æœ‰ OCR è¼¸å‡ºå«æœ‰æ–‡å­—ï¼Œå›å¡«ä¸¦æ¨™è¨˜ç‚ºå®Œæˆ: {local_path.name}")
                    item['parsed_text'] = validated_text
                    item['status'] = 'OCR_COMPLETED'
                    # è·³éç™¼é€æ–°çš„ OCR è«‹æ±‚
                    continue
                else:
                    logging.info(f"ç¾æœ‰ OCR è¼¸å‡ºæ²’æœ‰åµæ¸¬åˆ°æ–‡å­—ï¼Œå°‡é‡æ–°ç™¼é€ OCR è«‹æ±‚: {gcs_output_uri_base}")
            except Exception as e:
                logging.error(f"é©—è­‰ç¾æœ‰ OCR çµæœå¤±æ•— ID {item.get('id')}: {e}")
                # å¦‚æœé©—è­‰éç¨‹æœ‰éŒ¯èª¤ï¼Œæˆ‘å€‘ä»å˜—è©¦ç™¼é€æ–°çš„ OCR è«‹æ±‚

        operation = async_batch_annotate_file(vision_client, gcs_source_uri, gcs_output_uri_base)

        if operation:
            # å„²å­˜ operation è³‡è¨Šï¼Œä»¥ä¾¿å¾ŒçºŒæª¢æŸ¥ç‹€æ…‹
            operations.append({
                'id': item['id'],
                'name': item['name'],
                # å„²å­˜å¯¦éš›ä¸Šå‚³çš„æœ¬æ©Ÿè·¯å¾‘ï¼ˆå¯èƒ½æ˜¯åŸå§‹æª”ï¼Œæˆ–æ˜¯åŒåçš„ .pdfï¼‰
                'local_path': str(upload_path),
                'operation': operation,
                'gcs_output_uri': gcs_output_uri_base,
            })
            
    if not operations:
        logging.warning("æ²’æœ‰æˆåŠŸç™¼èµ· OCR æ“ä½œã€‚è«‹æª¢æŸ¥æ—¥èªŒã€‚")
        return

    logging.info(f"å·²ç™¼èµ· {len(operations)} å€‹ OCR è«‹æ±‚ã€‚ç­‰å¾…è™•ç†...")

    # 2b. ç­‰å¾… OCR å®Œæˆä¸¦ä¸‹è¼‰çµæœ
    
    # ç°¡æ˜“ç­‰å¾…è¿´åœˆ (å¯¦éš›æ‡‰ç”¨ä¸­æ‡‰ä½¿ç”¨æ›´ç²¾ç´°çš„é‡è©¦æ©Ÿåˆ¶)
    while operations:
        completed_operations = []
        logging.info(f"æª¢æŸ¥ OCR ç‹€æ…‹. å‰©é¤˜ {len(operations)} å€‹å¾…è™•ç†...")
        time.sleep(30) # æš«åœ 30 ç§’ç­‰å¾…è™•ç†

        # ä½¿ç”¨ operations client æ­£ç¢ºæŸ¥è©¢ long-running operation ç‹€æ…‹
        operations_client = vision_client.transport.operations_client

        for op_info in operations:
            # å…ˆå–å¾— operation çš„ nameï¼ˆå¯èƒ½åœ¨ä¸åŒ wrapper å±¬æ€§ä¸‹ï¼‰
            op_name = getattr(op_info['operation'], 'name', None)
            if op_name is None:
                op_name = getattr(getattr(op_info['operation'], 'operation', None), 'name', None)
            if not op_name:
                logging.warning(f"ç„¡æ³•å–å¾— operation åç¨±ï¼Œè·³é ID {op_info['id']}")
                continue

            try:
                op = operations_client.get_operation(op_name)
            except Exception as e:
                logging.error(f"æŸ¥è©¢ operation ç‹€æ…‹å¤±æ•— ID {op_info['id']}: {e}")
                continue

            if getattr(op, 'done', False):
                logging.info(f"âœ… OCR å®Œæˆ ID {op_info['id']} - {op_info['name']}")

                try:
                    # ä¸‹è¼‰ä¸¦åˆä½µ OCR æ–‡æœ¬
                    ocr_text = download_ocr_results(storage_client, op_info['gcs_output_uri'])

                    # 3. å›å¡«çµæœåˆ° parsed_data
                    # å„ªå…ˆä»¥ ID + NAME å®Œå…¨é…å°ï¼ˆåŒä¸€ id å¯èƒ½æœ‰å¤šå€‹æª”æ¡ˆï¼‰
                    matched = False
                    # helper: normalize names for robust comparison (unicodedata + casefold)
                    def _norm_name(s: str) -> str:
                        return unicodedata.normalize('NFC', (s or '')).strip().casefold()

                    for item in parsed_data:
                        try:
                            if str(item.get('id')) == str(op_info.get('id')):
                                name_a = _norm_name(item.get('name'))
                                name_b = _norm_name(op_info.get('name'))
                                # è‹¥ name æ¬„ä½ç›¸åŒï¼Œè¦–ç‚ºå®Œå…¨é…å°
                                if name_a and name_b and name_a == name_b:
                                    item['parsed_text'] = ocr_text
                                    item['status'] = 'OCR_COMPLETED'
                                    matched = True
                                    break
                        except Exception:
                            continue

                    # è‹¥æœªæ‰¾åˆ° id+name çš„å®Œå…¨é…å°ï¼Œå›é€€åˆ°è¼ƒå¯¬é¬†çš„é…å°ï¼ˆid æˆ–æª”æ¡ˆè·¯å¾‘åŒ¹é…ï¼‰
                    if not matched:
                        for item in parsed_data:
                            try:
                                if str(item.get('id')) == str(op_info.get('id')):
                                    item['parsed_text'] = ocr_text
                                    item['status'] = 'OCR_COMPLETED'
                                    matched = True
                                    break
                            except Exception:
                                pass

                            fp = item.get('file_path_local')
                            # ä¹Ÿæ¯”è¼ƒæ¨™æº–åŒ–å¾Œçš„åŒå PDF è·¯å¾‘
                            if fp:
                                try:
                                    if fp == op_info.get('local_path') or str(Path(fp).with_suffix('.pdf')) == op_info.get('local_path'):
                                        item['parsed_text'] = ocr_text
                                        item['status'] = 'OCR_COMPLETED'
                                        matched = True
                                        break
                                except Exception:
                                    continue

                    if not matched:
                        logging.warning(f"å›å¡«å¤±æ•—ï¼šæ‰¾ä¸åˆ°å°æ‡‰çš„ parsed_data æ¢ç›® (id={op_info.get('id')} name={op_info.get('name')} local={op_info.get('local_path')})")

                    completed_operations.append(op_info)

                except Exception as e:
                    logging.error(f"ä¸‹è¼‰æˆ–å›å¡«å¤±æ•— ID {op_info['id']}: {e}")
                    # å¦‚æœå¤±æ•—ï¼Œå°‡å…¶æ¨™è¨˜ç‚ºéŒ¯èª¤ï¼Œé¿å…é‡è¤‡é‡è©¦
                    # æ¨™è¨˜å¤±æ•—æ™‚ä¹Ÿä½¿ç”¨ç›¸åŒçš„é…å°é‚è¼¯ï¼šå„ªå…ˆ id+nameï¼Œå¦å‰‡å›é€€åˆ° id æˆ–è·¯å¾‘
                    failed_marked = False
                    def _norm_name(s: str) -> str:
                        return unicodedata.normalize('NFC', (s or '')).strip().casefold()

                    for item in parsed_data:
                        try:
                            if str(item.get('id')) == str(op_info.get('id')):
                                name_a = _norm_name(item.get('name'))
                                name_b = _norm_name(op_info.get('name'))
                                if name_a and name_b and name_a == name_b:
                                    item['status'] = 'OCR_FAILED'
                                    item['parsed_text'] = f"[OCR_FAILED: {e}]"
                                    failed_marked = True
                                    break
                        except Exception:
                            continue

                    if not failed_marked:
                        for item in parsed_data:
                            try:
                                if str(item.get('id')) == str(op_info.get('id')):
                                    item['status'] = 'OCR_FAILED'
                                    item['parsed_text'] = f"[OCR_FAILED: {e}]"
                                    failed_marked = True
                                    break
                            except Exception:
                                pass

                            fp = item.get('file_path_local')
                            if fp:
                                try:
                                    if fp == op_info.get('local_path') or str(Path(fp).with_suffix('.pdf')) == op_info.get('local_path'):
                                        item['status'] = 'OCR_FAILED'
                                        item['parsed_text'] = f"[OCR_FAILED: {e}]"
                                        failed_marked = True
                                        break
                                except Exception:
                                    continue

                    if not failed_marked:
                        logging.warning(f"ç„¡æ³•ç‚ºå¤±æ•—çš„ OCR æ“ä½œæ‰¾åˆ°å°æ‡‰çš„ parsed_data æ¢ç›® (id={op_info.get('id')} name={op_info.get('name')} local={op_info.get('local_path')})")
                    completed_operations.append(op_info)
        
        # å¾å¾…è™•ç†æ¸…å–®ä¸­ç§»é™¤å·²å®Œæˆçš„
        operations = [op for op in operations if op not in completed_operations]
        
    logging.info("æ‰€æœ‰ OCR æ“ä½œæª¢æŸ¥å®Œç•¢ã€‚")

    # 4. å„²å­˜æ›´æ–°å¾Œçš„ JSON (ä½¿ç”¨æ›´ç©©å¥çš„åŸå­å¯«å…¥æ©Ÿåˆ¶)
    # -----------------------------------------------
    # å¯«å…¥æµç¨‹ï¼š
    # 1) åœ¨ç›¸åŒç›®éŒ„å»ºç«‹ä¸€å€‹ NamedTemporaryFileï¼Œå¯«å…¥ä¸¦ fsync
    # 2) å°‡åŸå§‹æª”æ¡ˆå‚™ä»½ç‚º .bakï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    # 3) ç”¨ os.replace åŸå­æ€§åœ°å–ä»£åŸå§‹æª”æ¡ˆ
    # 4) å¯«å…¥å¤±æ•—æ™‚å˜—è©¦é‚„åŸå‚™ä»½ä¸¦æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
    import tempfile

    TEMP_OUTPUT_FILE = None
    BACKUP_FILE = PARSED_INPUT_FILE.with_suffix('.json.bak')

    try:
        logging.info(f"é–‹å§‹ç©©å¥åŸå­å¯«å…¥åˆ°è‡¨æ™‚æª”æ¡ˆ (dir={PARSED_INPUT_FILE.parent}): {PARSED_INPUT_FILE.name}.tmp")

        # åœ¨åŒä¸€å€‹ç›®éŒ„ä¸‹å»ºç«‹æš«å­˜æª”ï¼Œé¿å…è·¨æª”æ¡ˆç³»çµ± rename å•é¡Œ
        tf = tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', dir=str(PARSED_INPUT_FILE.parent), delete=False, prefix=PARSED_INPUT_FILE.stem + '-', suffix='.json.tmp')
        TEMP_OUTPUT_FILE = Path(tf.name)

        try:
            json.dump(parsed_data, tf, ensure_ascii=False, indent=4)
            tf.flush()
            os.fsync(tf.fileno())
        finally:
            tf.close()

        # å‚™ä»½åŸå§‹æª”æ¡ˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if PARSED_INPUT_FILE.exists():
            try:
                os.replace(PARSED_INPUT_FILE, BACKUP_FILE)
            except Exception as e:
                logging.warning(f"ç„¡æ³•å»ºç«‹å‚™ä»½ {BACKUP_FILE}: {e}")

        # åŸå­æ€§æ›¿æ›
        os.replace(str(TEMP_OUTPUT_FILE), str(PARSED_INPUT_FILE))

        # å¦‚æœæ›¿æ›æˆåŠŸï¼Œç§»é™¤å‚™ä»½ï¼ˆå¯é¸ï¼‰
        if BACKUP_FILE.exists():
            try:
                os.remove(BACKUP_FILE)
            except Exception:
                logging.debug(f"ç„¡æ³•åˆªé™¤å‚™ä»½æª”æ¡ˆ {BACKUP_FILE}ï¼ˆå¯å¿½ç•¥ï¼‰")

        logging.info(f"ğŸ‰ æ•¸æ“šé›†å·²å®‰å…¨æ›´æ–° OCR çµæœä¸¦ä¿å­˜åˆ° {PARSED_INPUT_FILE}")

    except Exception as e:
        logging.error(f"âŒ è­¦å‘Šï¼šç„¡æ³•é€²è¡Œç©©å¥åŸå­å¯«å…¥: {e}")
        # å˜—è©¦é‚„åŸå‚™ä»½ï¼ˆå¦‚æœå­˜åœ¨ä¸”ç›®æ¨™æª”æ¡ˆç¼ºå¤±ï¼‰
        try:
            if BACKUP_FILE.exists() and not PARSED_INPUT_FILE.exists():
                os.replace(BACKUP_FILE, PARSED_INPUT_FILE)
                logging.info(f"å·²é‚„åŸå‚™ä»½åˆ° {PARSED_INPUT_FILE}")
        except Exception as e2:
            logging.error(f"é‚„åŸå‚™ä»½å¤±æ•—: {e2}")

        # å˜—è©¦åˆªé™¤è‡¨æ™‚æª”æ¡ˆ
        try:
            if TEMP_OUTPUT_FILE and TEMP_OUTPUT_FILE.exists():
                os.remove(TEMP_OUTPUT_FILE)
        except Exception:
            pass

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Cloud OCR processor - optionally target specific IDs or filenames')
    parser.add_argument('--only-ids', help='Comma-separated list of IDs to process (e.g. 7669,7720)', default='')
    parser.add_argument('--only-names', help='Comma-separated list of filename fragments or basenames to process (e.g. æµ·å ±.jpg,å¡«å¯«ç¯„ä¾‹.jpg)', default='')
    args = parser.parse_args()

    only_ids = [x.strip() for x in args.only_ids.split(',') if x.strip()] if args.only_ids else None
    only_names = [x.strip() for x in args.only_names.split(',') if x.strip()] if args.only_names else None

    main_ocr_processor(only_ids=only_ids, only_names=only_names)
"""
é©—è­‰çå­¸é‡‘é™„ä»¶ä¸‹è¼‰çµæœçš„è…³æœ¬

æ­¤è…³æœ¬å°‡ï¼š
1. è¨ˆç®—æ‡‰ä¸‹è¼‰çš„é€£çµç¸½æ•¸ (N_link) æ ¹æ“šåŸå§‹è³‡æ–™æ–‡ä»¶
2. åˆ†æä¸‹è¼‰æ—¥èªŒæ–‡ä»¶ä»¥è¨ˆç®—å¯¦éš›æˆåŠŸèˆ‡å¤±æ•—çš„ä¸‹è¼‰æ•¸é‡
3. è¼¸å‡ºè©³ç´°çš„ä¸‹è¼‰å¤±æ•—æ¸…å–®ä»¥ä¾›é€²ä¸€æ­¥èª¿æŸ¥ 
"""


import pandas as pd
import re
import os
from pathlib import Path

# --- Configuration ---
# å‡è¨­æ‚¨çš„åŸå§‹è³‡æ–™æ–‡ä»¶
SCHOLARSHIPS_FILE = Path("data/raw/scholarships.csv")
# é™„åŠ æª”æ¡ˆæ¬„ä½åç¨±
ATTACHMENT_COL = 'é™„åŠ æª”æ¡ˆ'
# æ—¥èªŒæ–‡ä»¶è·¯å¾‘
LOG_FILE = Path("download_log.txt")
# ä¸‹è¼‰è³‡æ–™å¤¾è·¯å¾‘
ATTACHMENTS_DIR = Path("data/raw/attachments")
# ---------------------

# Regex to match URLs in the attachment column (to calculate N_link)
URL_REGEX = r'https?://[^\s\]]+'

# Regex to extract status from the log file
# Matches: Successfully downloaded {url} OR Failed to download {url}
SUCCESS_LOG_REGEX = r'Successfully downloaded (https?://.*?) to'
FAILURE_LOG_REGEX = r'Failed to download (https?://.*?) for scholarship (\w+?): (.*)'

def calculate_n_link(csv_path: Path, attachment_col: str) -> dict:
    """
    è¨ˆç®—æ‡‰ä¸‹è¼‰çš„é€£çµç¸½æ•¸ (N_link) ä¸¦å»ºç«‹é€£çµæ¸…å–®
    å›å‚³: {url: scholarship_id}
    """
    link_map = {}
    
    try:
        # å‡è¨­ CSV æ–‡ä»¶æ˜¯ä»¥ UTF-8 ç·¨ç¢¼å„²å­˜ (çˆ¬èŸ²è…³æœ¬çš„å¸¸è¦‹è¼¸å‡º)
        df = pd.read_csv(csv_path, encoding='utf-8')
    except FileNotFoundError:
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ {csv_path}")
        return {}
    except UnicodeDecodeError:
        print(f"éŒ¯èª¤ï¼šCSV æª”æ¡ˆ {csv_path} ç·¨ç¢¼éŒ¯èª¤ï¼Œå˜—è©¦ä½¿ç”¨ Big5 è®€å–...")
        try:
            df = pd.read_csv(csv_path, encoding='big5')
        except Exception as e:
            print(f"äºŒæ¬¡å˜—è©¦å¤±æ•—ï¼š{e}")
            return {}


    for index, row in df.iterrows():
        scholarship_id = row.get('ID', index)
        attachment_text = row.get(attachment_col)
        
        if pd.isna(attachment_text) or not attachment_text:
            continue
            
        # å°‹æ‰¾æ‰€æœ‰ URL
        links = re.findall(URL_REGEX, str(attachment_text))
        
        for url in links:
            # ä½¿ç”¨ URL ä½œç‚ºéµï¼Œç¢ºä¿æ¯å€‹é€£çµåªè¨ˆç®—ä¸€æ¬¡
            link_map[url] = scholarship_id
            
    return link_map

def analyze_log_file(log_path: Path) -> dict:
    """
    åˆ†ææ—¥èªŒæ–‡ä»¶ï¼Œè¨ˆç®—æˆåŠŸã€å¤±æ•—çš„ä¸‹è¼‰ï¼Œä¸¦è¨˜éŒ„å¤±æ•—è©³æƒ…
    å›å‚³: {
        'successful_urls': set,
        'failed_details': [(scholarship_id, url, reason), ...],
        'processed_urls_in_log': set
    }
    """
    successful_urls = set()
    failed_details = []
    processed_urls = set()

    try:
        # **** ä¿®å¾©é»ï¼šæ˜ç¢ºæŒ‡å®šç·¨ç¢¼ç‚º utf-8ï¼Œä¸¦å¿½ç•¥ç„¡æ³•è§£ç¢¼çš„å­—å…ƒ ****
        # é€™æ˜¯è™•ç† 'UnicodeDecodeError' çš„æ¨™æº–åšæ³•
        with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                # æª¢æŸ¥æˆåŠŸä¸‹è¼‰
                success_match = re.search(SUCCESS_LOG_REGEX, line)
                if success_match:
                    url = success_match.group(1).strip()
                    successful_urls.add(url)
                    processed_urls.add(url)
                    continue

                # æª¢æŸ¥ä¸‹è¼‰å¤±æ•—
                failure_match = re.search(FAILURE_LOG_REGEX, line)
                if failure_match:
                    url = failure_match.group(1).strip()
                    sch_id = failure_match.group(2).strip()
                    reason = failure_match.group(3).strip()
                    failed_details.append((sch_id, url, reason))
                    processed_urls.add(url)
                    continue

    except FileNotFoundError:
        print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°æ—¥èªŒæª”æ¡ˆ {log_path}ï¼Œç„¡æ³•é€²è¡Œè©³ç´°å¯©æ ¸ã€‚")
        return None

    return {
        'successful_urls': successful_urls,
        'failed_details': failed_details,
        'processed_urls_in_log': processed_urls
    }

def main_verification():
    """åŸ·è¡Œä¸‹è¼‰é©—è­‰ä¸»ç¨‹åº"""
    
    # 1. è¨ˆç®—æ‡‰ä¸‹è¼‰é€£çµç¸½æ•¸ (N_link)
    all_links = calculate_n_link(SCHOLARSHIPS_FILE, ATTACHMENT_COL)
    N_link = len(all_links)
    
    if N_link == 0:
        print("ç„¡æ³•è¨ˆç®—æ‡‰ä¸‹è¼‰é€£çµç¸½æ•¸ï¼Œè«‹ç¢ºèª CSV è·¯å¾‘åŠæ¬„ä½æ˜¯å¦æ­£ç¢ºã€‚")
        return

    # 2. åˆ†ææ—¥èªŒæ–‡ä»¶
    log_analysis = analyze_log_file(LOG_FILE)
    
    if not log_analysis:
        print(f"æ‡‰ä¸‹è¼‰é€£çµç¸½æ•¸ (N_link): {N_link}")
        print(f"--- çµæŸé©—è­‰ ---")
        return
        
    N_success = len(log_analysis['successful_urls'])
    N_fail = len(log_analysis['failed_details'])
    N_processed_log = len(log_analysis['processed_urls_in_log'])
    
    # 3. æ‰¾å‡ºéºæ¼çš„é€£çµ (Missing URLs)
    # ç†è«–ä¸Šï¼ŒN_link æ‡‰è©²ç­‰æ–¼ N_processed_log (æˆåŠŸ + å¤±æ•—ï¼Œä¸”ä¸é‡è¤‡)
    all_links_set = set(all_links.keys())
    
    # æœªå‡ºç¾åœ¨æ—¥èªŒä¸­çš„é€£çµ (å¯èƒ½æ˜¯çˆ¬èŸ²é‚è¼¯éŒ¯èª¤æˆ–è³‡æ–™å•é¡Œï¼Œä½†ä¸‹è¼‰è…³æœ¬æ²’æœ‰å˜—è©¦è™•ç†)
    unprocessed_urls = all_links_set - log_analysis['processed_urls_in_log']
    
    # 4. ç”¢ç”Ÿæœ€çµ‚å ±å‘Š
    
    print("\n" + "="*80)
    print("                ğŸ† çå­¸é‡‘é™„ä»¶ä¸‹è¼‰é©—è­‰å ±å‘Š (QUANTITATIVE) ğŸ†")
    print("="*80)
    print(f"  [1] æ‡‰è™•ç†é€£çµç¸½æ•¸ (N_link, æ ¹æ“š CSV): {N_link}")
    print(f"  [2] æ—¥èªŒè¨˜éŒ„çš„ä¸‹è¼‰å˜—è©¦ç¸½æ•¸:        {N_processed_log}")
    print("-" * 80)
    
    if N_link != N_processed_log:
        print(f"  âš ï¸ è­¦å‘Š: é€£çµæ•¸èˆ‡æ—¥èªŒè¨˜éŒ„æ•¸ä¸ç¬¦ ({N_link} vs {N_processed_log})ã€‚è«‹æª¢æŸ¥çˆ¬èŸ²é‚è¼¯ã€‚")
        
    print(f"  âœ… å¯¦éš›ä¸‹è¼‰æˆåŠŸæ•¸é‡:               {N_success}")
    print(f"  âŒ å¯¦éš›ä¸‹è¼‰å¤±æ•—æ•¸é‡ (æ—¥èªŒè¨˜éŒ„):      {N_fail}")
    
    # 5. è¼¸å‡ºå¤±æ•—è©³æƒ… (QUALITATIVE)
    if N_fail > 0:
        print("\n" + "="*30 + " âŒ è©³ç´°ä¸‹è¼‰å¤±æ•—æ¸…å–® " + "="*30)
        for sch_id, url, reason in log_analysis['failed_details']:
            print(f"  [ID: {sch_id}] URL: {url[:60]}... å¤±æ•—åŸå› : {reason}")
        
    # 6. è¼¸å‡ºéºæ¼æ¸…å–®
    if unprocessed_urls:
        print("\n" + "="*30 + " âš ï¸ æœªè™•ç†é€£çµæ¸…å–® (UNPROCESSED) " + "="*20)
        for url in unprocessed_urls:
             print(f"  [ID: {all_links[url]}] URL: {url[:60]}...")
             
    print("\n" + "="*80)
    print("ä¸‹ä¸€æ­¥å»ºè­°: å¦‚æœå¤±æ•—æ•¸é‡å¯æ¥å—ï¼Œè«‹é–‹å§‹é€²è¡Œæ–‡ä»¶è§£æ (æ–‡å­—æ“·å–)ã€‚")
    print("="*80)
    
    # é¡å¤–æª¢æŸ¥æª”æ¡ˆæ•¸é‡
    if ATTACHMENTS_DIR.exists():
        N_disk_file = sum(1 for item in ATTACHMENTS_DIR.iterdir() if item.is_file())
        print(f"\n (ç£ç¢Ÿæª”æ¡ˆæ•¸æª¢æŸ¥: {N_disk_file} å€‹æª”æ¡ˆå­˜æ–¼ {ATTACHMENTS_DIR.name}/)")
    else:
        print(f"\n (ç£ç¢Ÿæª”æ¡ˆæ•¸æª¢æŸ¥: æ‰¾ä¸åˆ°è³‡æ–™å¤¾ {ATTACHMENTS_DIR.name}/)")


if __name__ == "__main__":
    main_verification()